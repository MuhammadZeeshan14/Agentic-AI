{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install openai-agents SDK"
      ],
      "metadata": {
        "id": "PdKwzEluDBN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the openai-agents SDK using pip with the -Uq flags for upgrade and quiet output.\n",
        "!pip install -Uq openai-agents"
      ],
      "metadata": {
        "id": "3QdkOviEB2ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70405421-d2e5-4a45-e35b-494e970cf459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "7yD91lz4DIAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the nest_asyncio library, which allows running asyncio event loops in environments\n",
        "# where an event loop is already running, such as Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "# Apply nest_asyncio to the current event loop to patch asyncio and enable nested running.\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "7A5YLi3HCfBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set openai api keys"
      ],
      "metadata": {
        "id": "6QQn36cwuwvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries: userdata from google.colab for accessing secrets\n",
        "# and os for interacting with the operating system, specifically for setting environment variables.\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve the OpenAI API key stored in Colab's userdata secrets manager\n",
        "# and set it as an environment variable named \"OPENAI_API_KEY\".\n",
        "# This is a common practice for securely handling API keys.\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "VdB5Be_vuwE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set debug mode on (Optional)"
      ],
      "metadata": {
        "id": "sai9zXdAu4-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function `enable_verbose_stdout_logging` from the `agents` library.\n",
        "# This function is used to turn on detailed logging to the standard output\n",
        "# for the agents, which can be helpful for debugging.\n",
        "from agents import enable_verbose_stdout_logging\n",
        "\n",
        "# Call the function to enable verbose stdout logging for the agents.\n",
        "enable_verbose_stdout_logging()"
      ],
      "metadata": {
        "id": "bdWaK-w8u3dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run with OpenAI API"
      ],
      "metadata": {
        "id": "uTmzzUTUuHuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary classes from the agents library:\n",
        "# - Agent: Represents an AI agent with a specific role, instructions, and model.\n",
        "# - Runner: Provides methods to run agents synchronously or asynchronously.\n",
        "from agents import Agent, Runner\n",
        "\n",
        "# Import BaseModel from pydantic for defining data structures with type hints.\n",
        "# This is used here to define the expected structure of the agent's output.\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Define a Pydantic model named `ChatContext1` to specify the expected\n",
        "# format of the agent's output. It includes fields for name, role, and the last message.\n",
        "class ChatContext1(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "    last_message: str\n",
        "\n",
        "# Create an Agent instance.\n",
        "# - name: The name of the agent (set to None here).\n",
        "# - instructions: Instructions for the agent (set to None here).\n",
        "# - model: The language model to use. 'gpt-4.1-mini' is specified, but it might be a typo.\n",
        "#          A valid model name like 'gpt-4o-mini' or 'gpt-3.5-turbo' should be used.\n",
        "# - output_type: The expected structure of the agent's output, defined by the ChatContext1 Pydantic model.\n",
        "agent = Agent(name=None, instructions=None,\n",
        "              model='gpt-4.1-mini', # Potential typo in model name; consider using a valid model.\n",
        "              output_type=ChatContext1\n",
        "              )\n",
        "\n",
        "# Run the agent synchronously with the prompt \"Write a haiku about recursion in programming.\".\n",
        "# The Runner.run_sync() method blocks until the agent completes its task.\n",
        "result = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\n",
        "# Print the final structured output generated by the agent.\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNkquu4RuG4d",
        "outputId": "c733753c-d5e0-4a99-d13a-2c26663cefc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating trace Agent workflow with id trace_9f25842b65e5491e859d5274372750c5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Creating trace Agent workflow with id trace_9f25842b65e5491e859d5274372750c5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting current trace: trace_9f25842b65e5491e859d5274372750c5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Setting current trace: trace_9f25842b65e5491e859d5274372750c5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating span <agents.tracing.span_data.AgentSpanData object at 0x7d5526797d10> with id None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x7d5526797d10> with id None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent None (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent None (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating span <agents.tracing.span_data.ResponseSpanData object at 0x7d5526779540> with id None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Creating span <agents.tracing.span_data.ResponseSpanData object at 0x7d5526779540> with id None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LLM gpt-4.1-mini with input:\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Write a haiku about recursion in programming.\",\n",
            "    \"role\": \"user\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: {'format': {'type': 'json_schema', 'name': 'final_output', 'schema': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}, 'last_message': {'title': 'Last Message', 'type': 'string'}}, 'required': ['name', 'role', 'last_message'], 'title': 'ChatContext1', 'type': 'object', 'additionalProperties': False}, 'strict': True}}\n",
            "Previous response id: None\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling LLM gpt-4.1-mini with input:\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Write a haiku about recursion in programming.\",\n",
            "    \"role\": \"user\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: {'format': {'type': 'json_schema', 'name': 'final_output', 'schema': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}, 'last_message': {'title': 'Last Message', 'type': 'string'}}, 'required': ['name', 'role', 'last_message'], 'title': 'ChatContext1', 'type': 'object', 'additionalProperties': False}, 'strict': True}}\n",
            "Previous response id: None\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "[\n",
            "  {\n",
            "    \"id\": \"msg_6854288e95f481a193b1672bee870f9a0ab471ef24b3c3a7\",\n",
            "    \"content\": [\n",
            "      {\n",
            "        \"annotations\": [],\n",
            "        \"text\": \"{\\\"name\\\":\\\"HaikuBot\\\",\\\"role\\\":\\\"assistant\\\",\\\"last_message\\\":\\\"Endless calls that loop,\\\\nFunctions within functions dwell\\u2014\\\\nRecursion's soft dance.\\\"}\",\n",
            "        \"type\": \"output_text\",\n",
            "        \"logprobs\": null\n",
            "      }\n",
            "    ],\n",
            "    \"role\": \"assistant\",\n",
            "    \"status\": \"completed\",\n",
            "    \"type\": \"message\"\n",
            "  }\n",
            "]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "[\n",
            "  {\n",
            "    \"id\": \"msg_6854288e95f481a193b1672bee870f9a0ab471ef24b3c3a7\",\n",
            "    \"content\": [\n",
            "      {\n",
            "        \"annotations\": [],\n",
            "        \"text\": \"{\\\"name\\\":\\\"HaikuBot\\\",\\\"role\\\":\\\"assistant\\\",\\\"last_message\\\":\\\"Endless calls that loop,\\\\nFunctions within functions dwell\\u2014\\\\nRecursion's soft dance.\\\"}\",\n",
            "        \"type\": \"output_text\",\n",
            "        \"logprobs\": null\n",
            "      }\n",
            "    ],\n",
            "    \"role\": \"assistant\",\n",
            "    \"status\": \"completed\",\n",
            "    \"type\": \"message\"\n",
            "  }\n",
            "]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='HaikuBot' role='assistant' last_message=\"Endless calls that loop,\\nFunctions within functions dwell—\\nRecursion's soft dance.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemini with OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "K3VTUWDaGFcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries:\n",
        "# - os: For interacting with the operating system, like accessing environment variables.\n",
        "import os\n",
        "\n",
        "# Import classes and functions from the `agents` library:\n",
        "# - Agent: Represents an AI agent.\n",
        "# - Runner: Provides methods to run agents.\n",
        "# - AsyncOpenAI: A client for interacting with OpenAI-compatible APIs asynchronously.\n",
        "# - OpenAIChatCompletionsModel: A model wrapper for OpenAI-compatible chat completion models.\n",
        "# - RunConfig: A class to configure how an agent run should behave.\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "# Import userdata from google.colab to securely access stored secrets like API keys.\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "WBT9Z8hE6kEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Gemini API key from Colab's userdata secrets manager.\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Check if the retrieved API key is empty or None. If it is, raise a ValueError\n",
        "# instructing the user to set the GEMINI_API_KEY in their secrets or .env file.\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file or Colab secrets.\")\n",
        "\n",
        "# This comment provides a reference to the official Google documentation\n",
        "# on how to use the Gemini API with the OpenAI SDK, which is being done here.\n",
        "# Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "\n",
        "# Create an instance of AsyncOpenAI client.\n",
        "# - api_key: Set to the retrieved Gemini API key.\n",
        "# - base_url: Set to the specific endpoint for the Gemini API that is compatible with the OpenAI format.\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Create an instance of OpenAIChatCompletionsModel.\n",
        "# - model: Specify the name of the Gemini model to use (\"gemini-2.0-flash\").\n",
        "# - openai_client: Pass the configured external_client to this model wrapper,\n",
        "#                  so it uses the Gemini API endpoint.\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "# Create a RunConfig instance to customize the agent's execution settings.\n",
        "config = RunConfig(\n",
        "    model=model, # Set the model for this run configuration to the Gemini model.\n",
        "    model_provider=external_client, # Specify the model provider (the AsyncOpenAI client).\n",
        "    tracing_disabled=True # Disable tracing for this specific run configuration.\n",
        ")"
      ],
      "metadata": {
        "id": "QSIWS6RvC-a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hello world code | method one\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eTyY2o8yHqZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg583lQEBRWo",
        "outputId": "3b34b0a6-4f01-4f55-95cc-57f88f8415a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CALLING AGENT\n",
            "\n",
            "I am doing well, thank you for asking! How are you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create an Agent instance with a specific name, instructions, and the configured model.\n",
        "# - name: The name of the agent (\"Assistant\").\n",
        "# - instructions: The instructions for the agent (\"You are a helpful assistant\").\n",
        "# - model: The model instance configured to use the Gemini API (defined in the previous cell).\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\", model=model)\n",
        "\n",
        "# Run the agent synchronously using Runner.run_sync().\n",
        "# - agent: The agent instance to run.\n",
        "# - \"Hello, how are you.\": The user's prompt for the agent.\n",
        "# - run_config: The configuration for this run, specifying the model and disabling tracing.\n",
        "result = Runner.run_sync(agent, \"Hello, how are you.\", run_config=config)\n",
        "\n",
        "# Print a header to clearly separate the agent's output in the console.\n",
        "print(\"\\nCALLING AGENT\\n\")\n",
        "# Print the final output generated by the agent.\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hello world code | method two\n"
      ],
      "metadata": {
        "id": "p_x578x2JC0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries:\n",
        "# - asyncio: To work with asynchronous programming.\n",
        "import asyncio\n",
        "\n",
        "# Import Agent and Runner from the agents library.\n",
        "from agents import Agent, Runner\n",
        "\n",
        "# Define an asynchronous function named `main` where the agent execution will take place.\n",
        "async def main():\n",
        "    # Create an Agent instance.\n",
        "    # - name: The name of the agent (\"Assistant\").\n",
        "    # - instructions: Instructions for the agent, specifically to respond only in haikus.\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "    )\n",
        "\n",
        "    # Run the agent asynchronously using Runner.run().\n",
        "    # - agent: The agent instance.\n",
        "    # - \"Tell me about recursion in programming.\": The user's prompt.\n",
        "    # - run_config: The configuration for this run (using the 'config' defined earlier for Gemini).\n",
        "    # The `await` keyword is used because Runner.run() is an asynchronous function.\n",
        "    result = await Runner.run(agent, \"Tell me about recursion in programming.\",run_config=config)\n",
        "    # Print the final output generated by the agent.\n",
        "    print(result.final_output)\n",
        "    # Expected output (commented out) - This is an example of what a haiku about recursion might look like.\n",
        "    # Function calls itself,\n",
        "    # Looping in smaller pieces,\n",
        "    # Endless by design.\n",
        "\n",
        "\n",
        "# Check if the script is being run directly (not imported as a module).\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main asynchronous function using asyncio.run().\n",
        "    # This starts the asyncio event loop and runs the `main()` function until it completes.\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdkrSYWrH4CE",
        "outputId": "f09e8df5-f972-4167-ea26-1b0abb264426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A function calls self,\n",
            "Solving smaller problems now,\n",
            "Base case stops the flow.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries:\n",
        "# - asyncio: To work with asynchronous programming.\n",
        "import asyncio\n",
        "\n",
        "# Import Agent and Runner from the agents library.\n",
        "from agents import Agent, Runner\n",
        "\n",
        "# Define an asynchronous function named `main` where the agent execution will take place.\n",
        "async def main():\n",
        "    # Create an Agent instance with a name and instructions.\n",
        "    # - name: The name of the agent (\"Assistant\").\n",
        "    # - instructions: Instructions for the agent, specifically to respond only in haikus.\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "    )\n",
        "\n",
        "    # Run the agent and stream the results using Runner.run_streamed().\n",
        "    # This method returns a RunResultStreaming object which allows iterating over events as they occur.\n",
        "    # - agent: The agent instance.\n",
        "    # - \"Tell me about recursion in programming.\": The user's prompt.\n",
        "    # - run_config: The configuration for this run (using the 'config' defined earlier for Gemini).\n",
        "    result = Runner.run_streamed(agent, \"Tell me about recursion in programming.\",run_config=config)\n",
        "    # Print the initial RunResultStreaming object.\n",
        "    print(result)\n",
        "    # Iterate asynchronously through the stream events as they are generated by the agent.\n",
        "    # Each event represents a step or output from the agent's process.\n",
        "    async for e in result.stream_events():\n",
        "        # Print each event as it is received.\n",
        "        print(e)\n",
        "    # Expected output (commented out) - This is an example of what a haiku about recursion might look like.\n",
        "    # Function calls itself,\n",
        "    # Looping in smaller pieces,\n",
        "    # Endless by design.\n",
        "\n",
        "\n",
        "\n",
        "# Check if the script is being run directly (not imported as a module).\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main asynchronous function using asyncio.run().\n",
        "    # This starts the asyncio event loop and runs the `main()` function until it completes.\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFk51Np4jLqv",
        "outputId": "df8454c8-0454-4409-aacd-333c8267d172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResultStreaming:\n",
            "- Current agent: Agent(name=\"Assistant\", ...)\n",
            "- Current turn: 0\n",
            "- Max turns: 10\n",
            "- Is complete: False\n",
            "- Final output (NoneType):\n",
            "    None\n",
            "- 0 new item(s)\n",
            "- 0 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResultStreaming` for more details)\n",
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions='You only respond in haikus.', prompt=None, handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1751282440.0307698, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='A', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' function calls self', item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=',\\nBreaking down the problem small,\\nUntil base is met.\\n', item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='A function calls self,\\nBreaking down the problem small,\\nUntil base is met.\\n', type='output_text', logprobs=None), sequence_number=6, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='A function calls self,\\nBreaking down the problem small,\\nUntil base is met.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=7, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1751282440.0307698, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='A function calls self,\\nBreaking down the problem small,\\nUntil base is met.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=8, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions='You only respond in haikus.', prompt=None, handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='A function calls self,\\nBreaking down the problem small,\\nUntil base is met.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Level Custom model configuration"
      ],
      "metadata": {
        "id": "Sp50fXwl7gBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "        model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),\n",
        "    )\n",
        "\n",
        "```\n",
        "\n",
        "> **Note** `model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client)`"
      ],
      "metadata": {
        "id": "2OGs_oX688ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries:\n",
        "# - asyncio: To work with asynchronous programming.\n",
        "# - os: For interacting with the operating system, like accessing environment variables.\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Import the AsyncOpenAI client from the openai library.\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Import various classes and functions from the agents library:\n",
        "# - Agent: Represents an AI agent.\n",
        "# - OpenAIChatCompletionsModel: A model wrapper for OpenAI-compatible chat completion models.\n",
        "# - Runner: Provides methods to run agents.\n",
        "# - function_tool: A decorator to register a Python function as a tool for the agent.\n",
        "# - set_tracing_disabled: A function to disable tracing for agents globally.\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool, set_tracing_disabled\n",
        "\n",
        "# Get environment variables or use default values if the environment variables are not set.\n",
        "# - BASE_URL: The base URL for the API. Defaults to the Gemini API endpoint compatible with OpenAI.\n",
        "# - API_KEY: The API key. Attempts to get it from EXAMPLE_API_KEY env var or Colab's GEMINI_API_KEY userdata.\n",
        "# - MODEL_NAME: The name of the model to use. Defaults to \"gemini-2.0-flash\".\n",
        "BASE_URL = os.getenv(\"EXAMPLE_BASE_URL\") or \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "API_KEY = os.getenv(\"EXAMPLE_API_KEY\") or userdata.get(\"GEMINI_API_KEY\")\n",
        "MODEL_NAME = os.getenv(\"EXAMPLE_MODEL_NAME\") or \"gemini-2.0-flash\"\n",
        "\n",
        "# print(BASE_URL, API_KEY, MODEL_NAME) # Uncomment to print the values for debugging.\n",
        "\n",
        "# Check if the necessary variables (BASE_URL, API_KEY, MODEL_NAME) are set.\n",
        "# If any of them are missing, raise a ValueError with instructions on how to set them.\n",
        "if not BASE_URL or not API_KEY or not MODEL_NAME:\n",
        "    raise ValueError(\n",
        "        \"Please set EXAMPLE_BASE_URL, EXAMPLE_API_KEY, EXAMPLE_MODEL_NAME via env var or code.\"\n",
        "    )\n",
        "\n",
        "# Create an instance of AsyncOpenAI client with the specified base URL and API key.\n",
        "# This client will be used to interact with the Gemini API.\n",
        "client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
        "# Disable tracing for all agents globally.\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "# Define an asynchronous function named `main` where the agent execution will take place.\n",
        "async def main():\n",
        "    # Create an Agent instance.\n",
        "    # This agent is configured to use a custom LLM provider (the Gemini API via the OpenAI client).\n",
        "    # - name: The name of the agent (\"Assistant\").\n",
        "    # - instructions: Instructions for the agent, specifically to respond only in haikus.\n",
        "    # - model: Configure the agent to use the specific Gemini model and the custom client.\n",
        "    #          This overrides any global model configuration for this agent.\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "        # Configure the agent to use the specific Gemini model and client\n",
        "        model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),\n",
        "    )\n",
        "\n",
        "    # Run the agent asynchronously using Runner.run().\n",
        "    # - agent: The agent instance.\n",
        "    # - \"Who is the founder of Pakistan?\": The user's prompt.\n",
        "    # The agent will use the custom model configured at the agent level.\n",
        "    result = await Runner.run(agent, \"Who is the founder of Pakistan?\")\n",
        "    # Print the final output generated by the agent.\n",
        "    print(result.final_output)\n",
        "\n",
        "# Check if the script is being run directly (not imported as a module).\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main asynchronous function using asyncio.run().\n",
        "    # This starts the asyncio event loop and runs the `main()` function until it completes.\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DFZ0R5Y8VUh",
        "outputId": "f057d680-b91c-4998-e70f-dd5ad8691fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jinnah led the way,\n",
            "Pakistan's founding father,\n",
            "Nation born of will.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Model(LLM) configration on Global level\n",
        "> **Note**\n",
        "```python\n",
        "set_default_openai_client(client=client, use_for_tracing=False)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "set_tracing_disabled(disabled=True)\n",
        "```"
      ],
      "metadata": {
        "id": "xAofTw3g9js5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries:\n",
        "# - asyncio: To work with asynchronous programming.\n",
        "# - os: For interacting with the operating system, like accessing environment variables.\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Import the AsyncOpenAI client from the openai library.\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Import various classes and functions from the agents library:\n",
        "# - Agent: Represents an AI agent.\n",
        "# - Runner: Provides methods to run agents.\n",
        "# - function_tool: A decorator to register a Python function as a tool for the agent.\n",
        "# - set_default_openai_api: Function to set the default OpenAI API to use (e.g., chat completions).\n",
        "# - set_default_openai_client: Function to set the default OpenAI client to use globally.\n",
        "# - set_tracing_disabled: Function to disable tracing for agents globally.\n",
        "from agents import (\n",
        "    Agent,\n",
        "    Runner,\n",
        "    function_tool,\n",
        "    set_default_openai_api,\n",
        "    set_default_openai_client,\n",
        "    set_tracing_disabled,\n",
        ")\n",
        "\n",
        "# Get environment variables or use default values if the environment variables are not set.\n",
        "# - BASE_URL: The base URL for the API. Defaults to the Gemini API endpoint compatible with OpenAI.\n",
        "# - API_KEY: The API key. Attempts to get it from EXAMPLE_API_KEY env var or Colab's GEMINI_API_KEY userdata.\n",
        "# - MODEL_NAME: The name of the model to use. Defaults to \"gemini-2.0-flash\".\n",
        "BASE_URL = os.getenv(\"EXAMPLE_BASE_URL\") or \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "API_KEY = os.getenv(\"EXAMPLE_API_KEY\") or userdata.get(\"GEMINI_API_KEY\")\n",
        "MODEL_NAME = os.getenv(\"EXAMPLE_MODEL_NAME\") or \"gemini-2.0-flash\"\n",
        "\n",
        "# Check if the necessary variables (BASE_URL, API_KEY, MODEL_NAME) are set.\n",
        "# If any of them are missing, raise a ValueError with instructions on how to set them.\n",
        "if not BASE_URL or not API_KEY or not MODEL_NAME:\n",
        "    raise ValueError(\n",
        "        \"Please set EXAMPLE_BASE_URL, EXAMPLE_API_KEY, EXAMPLE_MODEL_NAME via env var or code.\"\n",
        "    )\n",
        "\n",
        "# Create an instance of AsyncOpenAI client with the specified base URL and API key.\n",
        "# This client will be used to interact with the Gemini API.\n",
        "client = AsyncOpenAI(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "# Set the default OpenAI client to the configured `client` instance.\n",
        "# This means any agent created without a specific client will use this one.\n",
        "# `use_for_tracing=False` indicates this client is not used for tracing purposes.\n",
        "set_default_openai_client(client=client, use_for_tracing=False)\n",
        "# Set the default OpenAI API type to use for chat completions.\n",
        "# Agents will default to using chat completions if not otherwise specified.\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "# Disable tracing for all agents globally.\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "# Define a function named `get_weather` and register it as a tool using the `@function_tool` decorator.\n",
        "# This function takes a `city` (string) as input.\n",
        "@function_tool\n",
        "def get_weather(city: str):\n",
        "    # Print a debug message indicating that the weather information is being retrieved for the specified city.\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    # Return a hardcoded string indicating the weather in the given city.\n",
        "    # In a real application, this would call an external weather service.\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "# Define an asynchronous function named `main` where the agent execution will take place.\n",
        "async def main():\n",
        "    # Create an Agent instance.\n",
        "    # - name: The name of the agent (\"Assistant\").\n",
        "    # - instructions: Instructions for the agent, specifically to respond only in haikus.\n",
        "    # - model: Set the model name. Since a default client and API are set globally,\n",
        "    #          this agent will use the globally configured Gemini model and client.\n",
        "    # - tools: Provide a list of tools available to the agent. Here, the `get_weather` function tool is provided.\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "        model=MODEL_NAME, # Use the globally configured default model\n",
        "        tools=[get_weather], # Provide the get_weather function as a tool\n",
        "    )\n",
        "\n",
        "    # Run the agent asynchronously using Runner.run().\n",
        "    # - agent: The agent instance.\n",
        "    # - \"What's the weather in Tokyo?\": The user's prompt.\n",
        "    # The agent might decide to use the `get_weather` tool to answer this question.\n",
        "    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n",
        "    # Print the final output generated by the agent.\n",
        "    print(result.final_output)\n",
        "\n",
        "# Check if the script is being run directly (not imported as a module).\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main asynchronous function using asyncio.run().\n",
        "    # This starts the asyncio event loop and runs the `main()` function until it completes.\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "pTNf1noxCRi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99f3685-6d37-43dd-9a7a-dad10dac2f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "City of Tokyo,\n",
            "Weather information sought,\n",
            "I will fetch for you.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}